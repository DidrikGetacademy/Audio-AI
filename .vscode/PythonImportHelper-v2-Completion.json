[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "stempeg",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "stempeg",
        "description": "stempeg",
        "detail": "stempeg",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "librosa",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "librosa",
        "description": "librosa",
        "detail": "librosa",
        "documentation": {}
    },
    {
        "label": "soundfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "soundfile",
        "description": "soundfile",
        "detail": "soundfile",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "MUSDB18StemDataset",
        "importPath": "Model.Data.dataset",
        "description": "Model.Data.dataset",
        "isExtraImport": true,
        "detail": "Model.Data.dataset",
        "documentation": {}
    },
    {
        "label": "MUSDB18StemDataset",
        "importPath": "Model.Data.dataset",
        "description": "Model.Data.dataset",
        "isExtraImport": true,
        "detail": "Model.Data.dataset",
        "documentation": {}
    },
    {
        "label": "MUSDB18StemDataset",
        "importPath": "Model.Data.dataset",
        "description": "Model.Data.dataset",
        "isExtraImport": true,
        "detail": "Model.Data.dataset",
        "documentation": {}
    },
    {
        "label": "MUSDB18StemDataset",
        "importPath": "Model.Data.dataset",
        "description": "Model.Data.dataset",
        "isExtraImport": true,
        "detail": "Model.Data.dataset",
        "documentation": {}
    },
    {
        "label": "UNet",
        "importPath": "Model.Model.model",
        "description": "Model.Model.model",
        "isExtraImport": true,
        "detail": "Model.Model.model",
        "documentation": {}
    },
    {
        "label": "UNet",
        "importPath": "Model.Model.model",
        "description": "Model.Model.model",
        "isExtraImport": true,
        "detail": "Model.Model.model",
        "documentation": {}
    },
    {
        "label": "UNet",
        "importPath": "Model.Model.model",
        "description": "Model.Model.model",
        "isExtraImport": true,
        "detail": "Model.Model.model",
        "documentation": {}
    },
    {
        "label": "UNet",
        "importPath": "Model.Model.model",
        "description": "Model.Model.model",
        "isExtraImport": true,
        "detail": "Model.Model.model",
        "documentation": {}
    },
    {
        "label": "UNet",
        "importPath": "Model.Model.model",
        "description": "Model.Model.model",
        "isExtraImport": true,
        "detail": "Model.Model.model",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "vgg16",
        "importPath": "torchvision.models",
        "description": "torchvision.models",
        "isExtraImport": true,
        "detail": "torchvision.models",
        "documentation": {}
    },
    {
        "label": "torchaudio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchaudio",
        "description": "torchaudio",
        "detail": "torchaudio",
        "documentation": {}
    },
    {
        "label": "onnxruntime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "onnxruntime",
        "description": "onnxruntime",
        "detail": "onnxruntime",
        "documentation": {}
    },
    {
        "label": "fine_tune_model",
        "importPath": "Model.Fine_tuning.Fine_Tuned_model",
        "description": "Model.Fine_tuning.Fine_Tuned_model",
        "isExtraImport": true,
        "detail": "Model.Fine_tuning.Fine_Tuned_model",
        "documentation": {}
    },
    {
        "label": "MUSDB18StemDataset",
        "kind": 6,
        "importPath": "Model.Data.dataset",
        "description": "Model.Data.dataset",
        "peekOfCode": "class MUSDB18StemDataset(Dataset):\n    def __init__(self,root_dir,subset='train',sr=44100,n_fft=1024,hop_length=256,max_length=10000, max_files=None):\n        self.root_dir = os.path.join(root_dir,subset) #Path to train/test folder\n        self.sr = sr #sampling rate\n        self.n_fft= n_fft #Number of FFT components\n        self.hop_length = hop_length #Hop length for STFT\n        self.max_length = max_length #fixed length for all spectrograms\n        self.file_paths = [\n            os.path.join(self.root_dir,file) #Collect all `.mp4` files in the folder.\n                           for file in os.listdir(self.root_dir)",
        "detail": "Model.Data.dataset",
        "documentation": {}
    },
    {
        "label": "load_audio",
        "kind": 2,
        "importPath": "Model.Evaluation.Evaluation",
        "description": "Model.Evaluation.Evaluation",
        "peekOfCode": "def load_audio(file_path, sr=44100, n_fft=1024, hop_length=256):\n    audio, _ = librosa.load(file_path, sr=sr, mono=True)\n    audio /= np.max(np.abs(audio))  # Normaliser til [-1, 1]\n    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n    magnitude = np.abs(stft)\n    return magnitude, stft\ndef reconstruct_audio(magnitude, phase, sr=44100, n_fft=1024, hop_length=256):\n    stft = magnitude * np.exp(1j * phase)  # Kombiner magnitude med fase\n    audio = librosa.istft(stft, hop_length=hop_length)\n    return audio",
        "detail": "Model.Evaluation.Evaluation",
        "documentation": {}
    },
    {
        "label": "reconstruct_audio",
        "kind": 2,
        "importPath": "Model.Evaluation.Evaluation",
        "description": "Model.Evaluation.Evaluation",
        "peekOfCode": "def reconstruct_audio(magnitude, phase, sr=44100, n_fft=1024, hop_length=256):\n    stft = magnitude * np.exp(1j * phase)  # Kombiner magnitude med fase\n    audio = librosa.istft(stft, hop_length=hop_length)\n    return audio\ndef evaluate_model(model_path, audio_file, output_file,  capcut_audio=None, sr=44100, n_fft=1024, hop_length=256):\n    # Last inn modellen\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = UNet(in_channels=1, out_channels=1).to(device)\n    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n    model.eval()",
        "detail": "Model.Evaluation.Evaluation",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "Model.Evaluation.Evaluation",
        "description": "Model.Evaluation.Evaluation",
        "peekOfCode": "def evaluate_model(model_path, audio_file, output_file,  capcut_audio=None, sr=44100, n_fft=1024, hop_length=256):\n    # Last inn modellen\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = UNet(in_channels=1, out_channels=1).to(device)\n    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n    model.eval()\n    # Last inn og prosesser input lydfil\n    input_magnitude, input_stft = load_audio(audio_file, sr, n_fft, hop_length)\n    input_phase = np.angle(input_stft)  # Fasen for Ã¥ rekonstruere senere\n    input_tensor = torch.tensor(input_magnitude, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)",
        "detail": "Model.Evaluation.Evaluation",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]",
        "kind": 5,
        "importPath": "Model.Evaluation.Evaluation",
        "description": "Model.Evaluation.Evaluation",
        "peekOfCode": "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nproject_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../..\"))\nsys.path.insert(0, project_root)  \nfrom Model.Data.dataset import MUSDB18StemDataset \nfrom Model.Model.model import UNet\ndef load_audio(file_path, sr=44100, n_fft=1024, hop_length=256):\n    audio, _ = librosa.load(file_path, sr=sr, mono=True)\n    audio /= np.max(np.abs(audio))  # Normaliser til [-1, 1]\n    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n    magnitude = np.abs(stft)",
        "detail": "Model.Evaluation.Evaluation",
        "documentation": {}
    },
    {
        "label": "project_root",
        "kind": 5,
        "importPath": "Model.Evaluation.Evaluation",
        "description": "Model.Evaluation.Evaluation",
        "peekOfCode": "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../..\"))\nsys.path.insert(0, project_root)  \nfrom Model.Data.dataset import MUSDB18StemDataset \nfrom Model.Model.model import UNet\ndef load_audio(file_path, sr=44100, n_fft=1024, hop_length=256):\n    audio, _ = librosa.load(file_path, sr=sr, mono=True)\n    audio /= np.max(np.abs(audio))  # Normaliser til [-1, 1]\n    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n    magnitude = np.abs(stft)\n    return magnitude, stft",
        "detail": "Model.Evaluation.Evaluation",
        "documentation": {}
    },
    {
        "label": "HybridLoss",
        "kind": 6,
        "importPath": "Model.Fine_tuning.Fine_Tuned_model",
        "description": "Model.Fine_tuning.Fine_Tuned_model",
        "peekOfCode": "class HybridLoss:\n    def __init__(self, device):\n        self.l1_loss = nn.L1Loss()\n        self.mse_loss = nn.MSELoss()\n        self.device = device\n        self.vgg = vgg16(pretrained=True).features[:5].to(device).eval()\n        for param in self.vgg.parameters():\n            param.requires_grad = False\n    def spectral_loss(self, output, target):\n        return torch.mean((output - target) ** 2)",
        "detail": "Model.Fine_tuning.Fine_Tuned_model",
        "documentation": {}
    },
    {
        "label": "freeze_encoder",
        "kind": 2,
        "importPath": "Model.Fine_tuning.Fine_Tuned_model",
        "description": "Model.Fine_tuning.Fine_Tuned_model",
        "peekOfCode": "def freeze_encoder(model):\n    for layer in model.encoder:\n        for param in layer.parameters():\n            param.requires_grad = False\n    print(\"Encoder layers frozen for fine-tuning.\")\nclass HybridLoss:\n    def __init__(self, device):\n        self.l1_loss = nn.L1Loss()\n        self.mse_loss = nn.MSELoss()\n        self.device = device",
        "detail": "Model.Fine_tuning.Fine_Tuned_model",
        "documentation": {}
    },
    {
        "label": "fine_tune_model",
        "kind": 2,
        "importPath": "Model.Fine_tuning.Fine_Tuned_model",
        "description": "Model.Fine_tuning.Fine_Tuned_model",
        "peekOfCode": "def fine_tune_model(pretrained_model_path, fine_tuned_model_path, root_dir, batch_size=2, learning_rate=1e-5, fine_tune_epochs=10):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Fine-tuning on device: {device}\")\n    print(\"Loading dataset...\")\n    dataset = MUSDB18StemDataset(\n        root_dir=root_dir, \n        subset='train', \n        sr=44100, \n        n_fft=2048, \n        hop_length=512,",
        "detail": "Model.Fine_tuning.Fine_Tuned_model",
        "documentation": {}
    },
    {
        "label": "project_root",
        "kind": 5,
        "importPath": "Model.Fine_tuning.Fine_Tuned_model",
        "description": "Model.Fine_tuning.Fine_Tuned_model",
        "peekOfCode": "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../..\"))\nsys.path.insert(0, project_root)\nfrom Model.Data.dataset import MUSDB18StemDataset\nfrom Model.Model.model import UNet\ndef freeze_encoder(model):\n    for layer in model.encoder:\n        for param in layer.parameters():\n            param.requires_grad = False\n    print(\"Encoder layers frozen for fine-tuning.\")\nclass HybridLoss:",
        "detail": "Model.Fine_tuning.Fine_Tuned_model",
        "documentation": {}
    },
    {
        "label": "create_and_save_model",
        "kind": 2,
        "importPath": "Model.Model.create_model",
        "description": "Model.Model.create_model",
        "peekOfCode": "def create_and_save_model(input_shape, in_channels=1, out_channels=1, save_path=\"unet_vocal_isolation.pth\"):\n    \"\"\"\n    Creates a U-Net model, validates it with a random tensor, and saves it.\n    Args:\n        input_shape (tuple): Shape of the input tensor (batch_size, channels, height, width).\n        in_channels (int): Number of input channels for the U-Net model.\n        out_channels (int): Number of output channels for the U-Net model.\n        save_path (str): Path to save the model state dictionary.\n    \"\"\"\n    # Select the device (GPU if available, else CPU)",
        "detail": "Model.Model.create_model",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]",
        "kind": 5,
        "importPath": "Model.Model.create_model",
        "description": "Model.Model.create_model",
        "peekOfCode": "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nproject_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../..\"))\nsys.path.insert(0, project_root)  # Use insert(0) to prioritize this path\nfrom Model.Model.model import UNet\ndef create_and_save_model(input_shape, in_channels=1, out_channels=1, save_path=\"unet_vocal_isolation.pth\"):\n    \"\"\"\n    Creates a U-Net model, validates it with a random tensor, and saves it.\n    Args:\n        input_shape (tuple): Shape of the input tensor (batch_size, channels, height, width).\n        in_channels (int): Number of input channels for the U-Net model.",
        "detail": "Model.Model.create_model",
        "documentation": {}
    },
    {
        "label": "project_root",
        "kind": 5,
        "importPath": "Model.Model.create_model",
        "description": "Model.Model.create_model",
        "peekOfCode": "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../..\"))\nsys.path.insert(0, project_root)  # Use insert(0) to prioritize this path\nfrom Model.Model.model import UNet\ndef create_and_save_model(input_shape, in_channels=1, out_channels=1, save_path=\"unet_vocal_isolation.pth\"):\n    \"\"\"\n    Creates a U-Net model, validates it with a random tensor, and saves it.\n    Args:\n        input_shape (tuple): Shape of the input tensor (batch_size, channels, height, width).\n        in_channels (int): Number of input channels for the U-Net model.\n        out_channels (int): Number of output channels for the U-Net model.",
        "detail": "Model.Model.create_model",
        "documentation": {}
    },
    {
        "label": "AttentionBlock",
        "kind": 6,
        "importPath": "Model.Model.model",
        "description": "Model.Model.model",
        "peekOfCode": "class AttentionBlock(nn.module):\n    def __init__(self, in_channels, gating_channels, inter_channels):\n        super(AttentionBlock, self).__init__()\n        self.W_g = nn.Conv2d(gating_channels, inter_channels, kernel_size=1, stride=1, padding=0, bias=True)\n        self.W_x = nn.conv2d(in_channels, inter_channels, kernel_size=1, stride=1, padding=0, bias=True)\n        self.psi = nn.Conv2d(inter_channels, 1, kernel_size=1, stride=1, padding=0, bias=True)\n        self.relu = nn.relU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self,  x, g):\n        g1 = self.W_g(g)",
        "detail": "Model.Model.model",
        "documentation": {}
    },
    {
        "label": "MultiScaleDecoderBlock",
        "kind": 6,
        "importPath": "Model.Model.model",
        "description": "Model.Model.model",
        "peekOfCode": "class MultiScaleDecoderBlock(nn.module):\n    def __init__(self, in_channels, out_channels):\nclass UNet(nn.Module): \n    def __init__(self, in_channels=1, out_channels=1, features=[16, 32, 64, 128]): \n        super(UNet, self).__init__()\n        self.encoder = nn.ModuleList()\n        for feature in features:\n            self.encoder.append(self.conv_block(in_channels, feature))\n            in_channels = feature\n        self.bottleneck = self.conv_block(features[-1], features[-1] * 2)",
        "detail": "Model.Model.model",
        "documentation": {}
    },
    {
        "label": "UNet",
        "kind": 6,
        "importPath": "Model.Model.model",
        "description": "Model.Model.model",
        "peekOfCode": "class UNet(nn.Module): \n    def __init__(self, in_channels=1, out_channels=1, features=[16, 32, 64, 128]): \n        super(UNet, self).__init__()\n        self.encoder = nn.ModuleList()\n        for feature in features:\n            self.encoder.append(self.conv_block(in_channels, feature))\n            in_channels = feature\n        self.bottleneck = self.conv_block(features[-1], features[-1] * 2)\n        self.decoder = nn.ModuleList()\n        reversed_features = list(reversed(features))",
        "detail": "Model.Model.model",
        "documentation": {}
    },
    {
        "label": "project_root",
        "kind": 5,
        "importPath": "Model.ONNX.Convert_to_ONNX",
        "description": "Model.ONNX.Convert_to_ONNX",
        "peekOfCode": "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../..\"))\nsys.path.insert(0, project_root)  # Use insert(0) to prioritize this path\nfrom Model.Data.dataset import MUSDB18StemDataset\nfrom Model.Model.model import UNet\nimport matplotlib.pyplot as plt\n# Load the dataset\nroot_dir = r\"C:\\mappe1\\musdb18\"\ndataset = MUSDB18StemDataset(root_dir=root_dir,subset=\"train\")\n# Convert tensor to NumPy for visualization\n# Get one sample from the dataset",
        "detail": "Model.ONNX.Convert_to_ONNX",
        "documentation": {}
    },
    {
        "label": "root_dir",
        "kind": 5,
        "importPath": "Model.ONNX.Convert_to_ONNX",
        "description": "Model.ONNX.Convert_to_ONNX",
        "peekOfCode": "root_dir = r\"C:\\mappe1\\musdb18\"\ndataset = MUSDB18StemDataset(root_dir=root_dir,subset=\"train\")\n# Convert tensor to NumPy for visualization\n# Get one sample from the dataset\nmixture_tensor, vocals_tensor = dataset[0]\nmixture_np = mixture_tensor.squeeze().numpy()\nplt.imshow(mixture_np, aspect=\"auto\", origin=\"lower\")\nplt.title(\"Mixture Spectrogram\")\nplt.colorbar()\nplt.show()",
        "detail": "Model.ONNX.Convert_to_ONNX",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "Model.ONNX.Convert_to_ONNX",
        "description": "Model.ONNX.Convert_to_ONNX",
        "peekOfCode": "dataset = MUSDB18StemDataset(root_dir=root_dir,subset=\"train\")\n# Convert tensor to NumPy for visualization\n# Get one sample from the dataset\nmixture_tensor, vocals_tensor = dataset[0]\nmixture_np = mixture_tensor.squeeze().numpy()\nplt.imshow(mixture_np, aspect=\"auto\", origin=\"lower\")\nplt.title(\"Mixture Spectrogram\")\nplt.colorbar()\nplt.show()\nprint(f\"Shape of mixture tensor: {mixture_tensor.shape}\")",
        "detail": "Model.ONNX.Convert_to_ONNX",
        "documentation": {}
    },
    {
        "label": "mixture_np",
        "kind": 5,
        "importPath": "Model.ONNX.Convert_to_ONNX",
        "description": "Model.ONNX.Convert_to_ONNX",
        "peekOfCode": "mixture_np = mixture_tensor.squeeze().numpy()\nplt.imshow(mixture_np, aspect=\"auto\", origin=\"lower\")\nplt.title(\"Mixture Spectrogram\")\nplt.colorbar()\nplt.show()\nprint(f\"Shape of mixture tensor: {mixture_tensor.shape}\")\nprint(f\"Shape of vocals tensor: {vocals_tensor.shape}\")\n# Reshape the tensor for batch dimension\ninput_tensor = mixture_tensor.unsqueeze(0)\n#Initialize the model",
        "detail": "Model.ONNX.Convert_to_ONNX",
        "documentation": {}
    },
    {
        "label": "input_tensor",
        "kind": 5,
        "importPath": "Model.ONNX.Convert_to_ONNX",
        "description": "Model.ONNX.Convert_to_ONNX",
        "peekOfCode": "input_tensor = mixture_tensor.unsqueeze(0)\n#Initialize the model\nmodel = UNet(in_channels=1,out_channels=1)\nmodel.load_state_dict(torch.load(r\"C:\\Users\\didri\\Desktop\\AI AudioEnchancer\\UNet_Model\\CheckPoints\\unet_checkpoint_epoch30.pth\",weights_only=True))\nmodel.eval()\n#Save to ONNX\nonnx_path = r\"C:\\Users\\didri\\Desktop\\AI AudioEnchancer\\UNet_Model\\ONNX_model\\ONNX.onnx\"\ntorch.onnx.export(\n    model,\n    input_tensor,",
        "detail": "Model.ONNX.Convert_to_ONNX",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Model.ONNX.Convert_to_ONNX",
        "description": "Model.ONNX.Convert_to_ONNX",
        "peekOfCode": "model = UNet(in_channels=1,out_channels=1)\nmodel.load_state_dict(torch.load(r\"C:\\Users\\didri\\Desktop\\AI AudioEnchancer\\UNet_Model\\CheckPoints\\unet_checkpoint_epoch30.pth\",weights_only=True))\nmodel.eval()\n#Save to ONNX\nonnx_path = r\"C:\\Users\\didri\\Desktop\\AI AudioEnchancer\\UNet_Model\\ONNX_model\\ONNX.onnx\"\ntorch.onnx.export(\n    model,\n    input_tensor,\n    onnx_path,\n    export_params=True,",
        "detail": "Model.ONNX.Convert_to_ONNX",
        "documentation": {}
    },
    {
        "label": "onnx_path",
        "kind": 5,
        "importPath": "Model.ONNX.Convert_to_ONNX",
        "description": "Model.ONNX.Convert_to_ONNX",
        "peekOfCode": "onnx_path = r\"C:\\Users\\didri\\Desktop\\AI AudioEnchancer\\UNet_Model\\ONNX_model\\ONNX.onnx\"\ntorch.onnx.export(\n    model,\n    input_tensor,\n    onnx_path,\n    export_params=True,\n    opset_version=11,\n    do_constant_folding=True,\n    input_names=[\"input\"], \n    output_names=[\"output\"],",
        "detail": "Model.ONNX.Convert_to_ONNX",
        "documentation": {}
    },
    {
        "label": "onnx_model_path",
        "kind": 5,
        "importPath": "Model.ONNX.Test_Converted_ONNX",
        "description": "Model.ONNX.Test_Converted_ONNX",
        "peekOfCode": "onnx_model_path = r\"C:\\Users\\didri\\Desktop\\AI AudioEnchancer\\UNet_Model\\ONNX_model\\ONNX.onnx\"\nif not os.path.exists(onnx_model_path):\n    raise FileNotFoundError(f\"ONNX model not found at {onnx_model_path}\")\nsession = ort.InferenceSession(onnx_model_path)\nsession.set_providers(['CUDAExecutionProvider'])\n# Print information about the model's input nodes\nprint(\"\\n--- Model Input Nodes ---\")\ninput_nodes = session.get_inputs()\nfor input_node in input_nodes:\n    print(f\"Name: {input_node.name}, Shape: {input_node.shape}, Type: {input_node.type}\")",
        "detail": "Model.ONNX.Test_Converted_ONNX",
        "documentation": {}
    },
    {
        "label": "session",
        "kind": 5,
        "importPath": "Model.ONNX.Test_Converted_ONNX",
        "description": "Model.ONNX.Test_Converted_ONNX",
        "peekOfCode": "session = ort.InferenceSession(onnx_model_path)\nsession.set_providers(['CUDAExecutionProvider'])\n# Print information about the model's input nodes\nprint(\"\\n--- Model Input Nodes ---\")\ninput_nodes = session.get_inputs()\nfor input_node in input_nodes:\n    print(f\"Name: {input_node.name}, Shape: {input_node.shape}, Type: {input_node.type}\")\n# Print information about the model's output nodes\nprint(\"\\n--- Model Output Nodes ---\")\noutput_nodes = session.get_outputs()",
        "detail": "Model.ONNX.Test_Converted_ONNX",
        "documentation": {}
    },
    {
        "label": "input_nodes",
        "kind": 5,
        "importPath": "Model.ONNX.Test_Converted_ONNX",
        "description": "Model.ONNX.Test_Converted_ONNX",
        "peekOfCode": "input_nodes = session.get_inputs()\nfor input_node in input_nodes:\n    print(f\"Name: {input_node.name}, Shape: {input_node.shape}, Type: {input_node.type}\")\n# Print information about the model's output nodes\nprint(\"\\n--- Model Output Nodes ---\")\noutput_nodes = session.get_outputs()\nfor output_node in output_nodes:\n    print(f\"Name: {output_node.name}, Shape: {output_node.shape}, Type: {output_node.type}\")\n# Show available execution providers\nprint(\"\\n--- Available Providers ---\")",
        "detail": "Model.ONNX.Test_Converted_ONNX",
        "documentation": {}
    },
    {
        "label": "output_nodes",
        "kind": 5,
        "importPath": "Model.ONNX.Test_Converted_ONNX",
        "description": "Model.ONNX.Test_Converted_ONNX",
        "peekOfCode": "output_nodes = session.get_outputs()\nfor output_node in output_nodes:\n    print(f\"Name: {output_node.name}, Shape: {output_node.shape}, Type: {output_node.type}\")\n# Show available execution providers\nprint(\"\\n--- Available Providers ---\")\nprint(f\"Devices supported: {session.get_providers()}\")\n# Uncomment to set CUDA provider if available\n#if 'CUDAExecutionProvider' in session.get_providers():\n#    session.set_providers(['CUDAExecutionProvider'])\n# Get input and output node names",
        "detail": "Model.ONNX.Test_Converted_ONNX",
        "documentation": {}
    },
    {
        "label": "input_name",
        "kind": 5,
        "importPath": "Model.ONNX.Test_Converted_ONNX",
        "description": "Model.ONNX.Test_Converted_ONNX",
        "peekOfCode": "input_name = session.get_inputs()[0].name\noutput_name = session.get_outputs()[0].name\nprint(f\"\\nInput Name: {input_name}, Shape: {session.get_inputs()[0].shape}\")\nprint(f\"Output Name: {output_name}, Shape: {session.get_outputs()[0].shape}\")\n# Load an audio file and dynamically compute its spectrogram\naudio_path = r\"C:\\Users\\didri\\Desktop\\AI AudioEnchancer\\Model\\Data\\WAV_files_for_model\\Capcut_mixed\\withsound(1).WAV\"\nif not os.path.exists(audio_path):\n    raise FileNotFoundError(f\"Audio file not found at {audio_path}\")\nprint(\"\\nLoading audio and generating spectrogram...\")\ny, sr = librosa.load(audio_path, sr=44100)  # Load audio at 44.1kHz",
        "detail": "Model.ONNX.Test_Converted_ONNX",
        "documentation": {}
    },
    {
        "label": "output_name",
        "kind": 5,
        "importPath": "Model.ONNX.Test_Converted_ONNX",
        "description": "Model.ONNX.Test_Converted_ONNX",
        "peekOfCode": "output_name = session.get_outputs()[0].name\nprint(f\"\\nInput Name: {input_name}, Shape: {session.get_inputs()[0].shape}\")\nprint(f\"Output Name: {output_name}, Shape: {session.get_outputs()[0].shape}\")\n# Load an audio file and dynamically compute its spectrogram\naudio_path = r\"C:\\Users\\didri\\Desktop\\AI AudioEnchancer\\Model\\Data\\WAV_files_for_model\\Capcut_mixed\\withsound(1).WAV\"\nif not os.path.exists(audio_path):\n    raise FileNotFoundError(f\"Audio file not found at {audio_path}\")\nprint(\"\\nLoading audio and generating spectrogram...\")\ny, sr = librosa.load(audio_path, sr=44100)  # Load audio at 44.1kHz\nspectrogram = np.abs(librosa.stft(y, n_fft=2048, hop_length=512))",
        "detail": "Model.ONNX.Test_Converted_ONNX",
        "documentation": {}
    },
    {
        "label": "audio_path",
        "kind": 5,
        "importPath": "Model.ONNX.Test_Converted_ONNX",
        "description": "Model.ONNX.Test_Converted_ONNX",
        "peekOfCode": "audio_path = r\"C:\\Users\\didri\\Desktop\\AI AudioEnchancer\\Model\\Data\\WAV_files_for_model\\Capcut_mixed\\withsound(1).WAV\"\nif not os.path.exists(audio_path):\n    raise FileNotFoundError(f\"Audio file not found at {audio_path}\")\nprint(\"\\nLoading audio and generating spectrogram...\")\ny, sr = librosa.load(audio_path, sr=44100)  # Load audio at 44.1kHz\nspectrogram = np.abs(librosa.stft(y, n_fft=2048, hop_length=512))\nprint(f\"Original Spectrogram Shape: {spectrogram.shape}\")\n# Prepare input tensor\nspectrogram = spectrogram.astype(np.float32)\nspectrogram = np.expand_dims(spectrogram, axis=(0, 1))  # Add batch and channel dimensions",
        "detail": "Model.ONNX.Test_Converted_ONNX",
        "documentation": {}
    },
    {
        "label": "spectrogram",
        "kind": 5,
        "importPath": "Model.ONNX.Test_Converted_ONNX",
        "description": "Model.ONNX.Test_Converted_ONNX",
        "peekOfCode": "spectrogram = np.abs(librosa.stft(y, n_fft=2048, hop_length=512))\nprint(f\"Original Spectrogram Shape: {spectrogram.shape}\")\n# Prepare input tensor\nspectrogram = spectrogram.astype(np.float32)\nspectrogram = np.expand_dims(spectrogram, axis=(0, 1))  # Add batch and channel dimensions\nprint(f\"Prepared Input Tensor Shape: {spectrogram.shape}\")\n# Run inference\ntry:\n    print(\"\\nRunning ONNX inference...\")\n    result = session.run([output_name], {input_name: spectrogram})",
        "detail": "Model.ONNX.Test_Converted_ONNX",
        "documentation": {}
    },
    {
        "label": "spectrogram",
        "kind": 5,
        "importPath": "Model.ONNX.Test_Converted_ONNX",
        "description": "Model.ONNX.Test_Converted_ONNX",
        "peekOfCode": "spectrogram = spectrogram.astype(np.float32)\nspectrogram = np.expand_dims(spectrogram, axis=(0, 1))  # Add batch and channel dimensions\nprint(f\"Prepared Input Tensor Shape: {spectrogram.shape}\")\n# Run inference\ntry:\n    print(\"\\nRunning ONNX inference...\")\n    result = session.run([output_name], {input_name: spectrogram})\n    print(\"ONNX Inference Successful\")\nexcept Exception as e:\n    print(f\"Error during ONNX inference: {e}\")",
        "detail": "Model.ONNX.Test_Converted_ONNX",
        "documentation": {}
    },
    {
        "label": "spectrogram",
        "kind": 5,
        "importPath": "Model.ONNX.Test_Converted_ONNX",
        "description": "Model.ONNX.Test_Converted_ONNX",
        "peekOfCode": "spectrogram = np.expand_dims(spectrogram, axis=(0, 1))  # Add batch and channel dimensions\nprint(f\"Prepared Input Tensor Shape: {spectrogram.shape}\")\n# Run inference\ntry:\n    print(\"\\nRunning ONNX inference...\")\n    result = session.run([output_name], {input_name: spectrogram})\n    print(\"ONNX Inference Successful\")\nexcept Exception as e:\n    print(f\"Error during ONNX inference: {e}\")\n    raise",
        "detail": "Model.ONNX.Test_Converted_ONNX",
        "documentation": {}
    },
    {
        "label": "output_spectrogram",
        "kind": 5,
        "importPath": "Model.ONNX.Test_Converted_ONNX",
        "description": "Model.ONNX.Test_Converted_ONNX",
        "peekOfCode": "output_spectrogram = np.squeeze(result[0])  # Remove batch and channel dimensions\nprint(f\"Output Spectrogram Shape: {output_spectrogram.shape}\")\nplt.figure(figsize=(10, 5))\nplt.imshow(output_spectrogram, aspect=\"auto\", origin=\"lower\")\nplt.title(\"Vocal Isolation Output Spectrogram\")\nplt.colorbar()\nplt.xlabel(\"Time\")\nplt.ylabel(\"Frequency\")\nplt.show()",
        "detail": "Model.ONNX.Test_Converted_ONNX",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "Model.Training.train",
        "description": "Model.Training.train",
        "peekOfCode": "def train(load_model_path=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device {device}\")\n    #HyperParameters\n    batch_size = 3\n    learning_rate = 1e-5\n    epochs = 25\n    root_dir = r'C:\\mappe1\\musdb18'\n    #Dataset and DataLoader\n    dataset = MUSDB18StemDataset(",
        "detail": "Model.Training.train",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Model.Training.train",
        "description": "Model.Training.train",
        "peekOfCode": "def test(model,test_loader,device):\n    print(\"Evaluating the model on the test set...\")\n    model.eval()\n    criterion = nn.MSELoss()\n    test_loss = 0.0\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs,targets = inputs.to(device,non_blocking=True), targets.to(device,non_blocking=True)\n            outputs = model(inputs)\n            loss = criterion(outputs,targets)",
        "detail": "Model.Training.train",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]",
        "kind": 5,
        "importPath": "Model.Training.train",
        "description": "Model.Training.train",
        "peekOfCode": "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nproject_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../..\"))\nsys.path.insert(0, project_root)  \nfrom Model.Data.dataset import MUSDB18StemDataset \nfrom Model.Model.model import UNet\nfrom Model.Fine_tuning.Fine_Tuned_model import fine_tune_model\ndef train(load_model_path=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device {device}\")\n    #HyperParameters",
        "detail": "Model.Training.train",
        "documentation": {}
    },
    {
        "label": "project_root",
        "kind": 5,
        "importPath": "Model.Training.train",
        "description": "Model.Training.train",
        "peekOfCode": "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../..\"))\nsys.path.insert(0, project_root)  \nfrom Model.Data.dataset import MUSDB18StemDataset \nfrom Model.Model.model import UNet\nfrom Model.Fine_tuning.Fine_Tuned_model import fine_tune_model\ndef train(load_model_path=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device {device}\")\n    #HyperParameters\n    batch_size = 3",
        "detail": "Model.Training.train",
        "documentation": {}
    }
]