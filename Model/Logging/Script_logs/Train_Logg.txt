2024-12-30 18:16:31,664 - INFO - [Train] Using device: cuda
2024-12-30 18:16:31,666 - INFO - [Train] Dataset => max_length: 3000, sr: 44100, n_fft: 2048, hop_length: 512, total_files: 100
2024-12-30 18:16:31,666 - INFO - [Train] Dataset => max_length: 3000, sr: 44100, n_fft: 2048, hop_length: 512, total_files: 100
2024-12-30 18:16:31,667 - INFO - [Eval] Dataset => max_length: 3000, sr: 44100, n_fft: 2048, hop_length: 512, total_files: 50
2024-12-30 18:16:33,452 - INFO - [Train] Starting => Batch_size=2, LR=1e-05, Epochs=10
2024-12-30 18:16:33,452 - INFO - [Train] Epoch 1/10 started.
2024-12-30 18:17:39,573 - INFO - [Train] Epoch 1, Batch 1: Loss=0.039700
2024-12-30 18:17:40,118 - INFO - [Train] Epoch 1, Batch 2: Loss=0.029610
2024-12-30 18:17:40,614 - INFO - [Train] Epoch 1, Batch 3: Loss=0.059525
2024-12-30 18:17:41,150 - INFO - [Train] Epoch 1, Batch 4: Loss=0.054682
2024-12-30 18:17:41,629 - INFO - [Train] Epoch 1, Batch 5: Loss=0.039703
2024-12-30 18:17:42,116 - INFO - [Train] Epoch 1, Batch 6: Loss=0.035918
2024-12-30 18:17:42,611 - INFO - [Train] Epoch 1, Batch 7: Loss=0.037456
2024-12-30 18:17:43,143 - INFO - [Train] Epoch 1, Batch 8: Loss=0.039203
2024-12-30 18:17:51,674 - INFO - [Train] Epoch 1, Batch 9: Loss=0.056396
2024-12-30 18:17:55,761 - INFO - [Train] Epoch 1, Batch 10: Loss=0.056100
2024-12-30 18:17:56,263 - INFO - [Train] Epoch 1, Batch 11: Loss=0.042186
2024-12-30 18:17:56,803 - INFO - [Train] Epoch 1, Batch 12: Loss=0.029435
2024-12-30 18:17:57,312 - INFO - [Train] Epoch 1, Batch 13: Loss=0.051614
2024-12-30 18:17:57,856 - INFO - [Train] Epoch 1, Batch 14: Loss=0.029432
2024-12-30 18:17:58,389 - INFO - [Train] Epoch 1, Batch 15: Loss=0.062136
2024-12-30 18:17:58,862 - INFO - [Train] Epoch 1, Batch 16: Loss=0.061533
2024-12-30 18:18:16,359 - INFO - [Train] Epoch 1, Batch 17: Loss=0.036150
2024-12-30 18:18:16,857 - INFO - [Train] Epoch 1, Batch 18: Loss=0.045243
2024-12-30 18:18:17,414 - INFO - [Train] Epoch 1, Batch 19: Loss=0.043030
2024-12-30 18:18:18,015 - INFO - [Train] Epoch 1, Batch 20: Loss=0.050538
2024-12-30 18:18:18,906 - INFO - [Train] Epoch 1, Batch 21: Loss=0.060846
2024-12-30 18:18:19,434 - INFO - [Train] Epoch 1, Batch 22: Loss=0.034717
2024-12-30 18:18:19,999 - INFO - [Train] Epoch 1, Batch 23: Loss=0.037617
2024-12-30 18:18:20,572 - INFO - [Train] Epoch 1, Batch 24: Loss=0.053842
2024-12-30 18:18:30,370 - INFO - [Train] Epoch 1, Batch 25: Loss=0.055815
2024-12-30 18:18:30,856 - INFO - [Train] Epoch 1, Batch 26: Loss=0.066106
2024-12-30 18:18:31,328 - INFO - [Train] Epoch 1, Batch 27: Loss=0.051259
2024-12-30 18:18:44,081 - INFO - [Train] Epoch 1, Batch 28: Loss=0.062461
2024-12-30 18:18:44,612 - INFO - [Train] Epoch 1, Batch 29: Loss=0.062293
2024-12-30 18:18:45,111 - INFO - [Train] Epoch 1, Batch 30: Loss=0.067871
2024-12-30 18:18:45,572 - INFO - [Train] Epoch 1, Batch 31: Loss=0.055188
2024-12-30 18:18:46,043 - INFO - [Train] Epoch 1, Batch 32: Loss=0.035681
2024-12-30 18:18:46,508 - INFO - [Train] Epoch 1, Batch 33: Loss=0.142583
2024-12-30 18:18:54,220 - INFO - [Train] Epoch 1, Batch 34: Loss=0.029347
2024-12-30 18:18:54,769 - INFO - [Train] Epoch 1, Batch 35: Loss=0.048020
2024-12-30 18:19:10,576 - INFO - [Train] Epoch 1, Batch 36: Loss=0.049475
2024-12-30 18:19:11,087 - INFO - [Train] Epoch 1, Batch 37: Loss=0.041690
2024-12-30 18:19:11,587 - INFO - [Train] Epoch 1, Batch 38: Loss=0.053036
2024-12-30 18:19:12,129 - INFO - [Train] Epoch 1, Batch 39: Loss=0.055486
2024-12-30 18:19:12,613 - INFO - [Train] Epoch 1, Batch 40: Loss=0.050390
2024-12-30 18:19:13,087 - INFO - [Train] Epoch 1, Batch 41: Loss=0.034182
2024-12-30 18:19:13,572 - INFO - [Train] Epoch 1, Batch 42: Loss=0.052339
2024-12-30 18:19:14,061 - INFO - [Train] Epoch 1, Batch 43: Loss=0.046918
2024-12-30 18:19:28,038 - INFO - [Train] Epoch 1, Batch 44: Loss=0.050236
2024-12-30 18:19:28,474 - INFO - [Train] Epoch 1, Batch 45: Loss=0.046216
2024-12-30 18:19:28,897 - INFO - [Train] Epoch 1, Batch 46: Loss=0.050697
2024-12-30 18:19:29,320 - INFO - [Train] Epoch 1, Batch 47: Loss=0.056541
2024-12-30 18:19:29,759 - INFO - [Train] Epoch 1, Batch 48: Loss=0.131852
2024-12-30 18:19:30,182 - INFO - [Train] Epoch 1, Batch 49: Loss=0.077019
2024-12-30 18:19:30,615 - INFO - [Train] Epoch 1, Batch 50: Loss=0.070282
2024-12-30 18:19:32,613 - INFO - [Train] Epoch 1 completed -> Avg Loss: 0.052592
2024-12-30 18:19:32,613 - INFO - [Train] Epoch 1 completed -> Avg Loss: 0.052592
2024-12-30 18:19:32,647 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_1.pth
2024-12-30 18:19:32,647 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_1.pth
2024-12-30 18:19:32,648 - INFO - [Train] Epoch 2/10 started.
2024-12-30 18:20:08,966 - INFO - [Train] Epoch 2, Batch 1: Loss=0.058550
2024-12-30 18:20:09,458 - INFO - [Train] Epoch 2, Batch 2: Loss=0.034170
2024-12-30 18:20:15,649 - INFO - [Train] Epoch 2, Batch 3: Loss=0.035221
2024-12-30 18:20:16,138 - INFO - [Train] Epoch 2, Batch 4: Loss=0.034303
2024-12-30 18:20:16,669 - INFO - [Train] Epoch 2, Batch 5: Loss=0.038386
2024-12-30 18:20:17,204 - INFO - [Train] Epoch 2, Batch 6: Loss=0.043587
2024-12-30 18:20:17,693 - INFO - [Train] Epoch 2, Batch 7: Loss=0.045533
2024-12-30 18:20:18,137 - INFO - [Train] Epoch 2, Batch 8: Loss=0.151804
2024-12-30 18:20:26,455 - INFO - [Train] Epoch 2, Batch 9: Loss=0.046132
2024-12-30 18:20:26,947 - INFO - [Train] Epoch 2, Batch 10: Loss=0.040131
2024-12-30 18:20:44,025 - INFO - [Train] Epoch 2, Batch 11: Loss=0.057027
2024-12-30 18:20:44,534 - INFO - [Train] Epoch 2, Batch 12: Loss=0.048814
2024-12-30 18:20:45,039 - INFO - [Train] Epoch 2, Batch 13: Loss=0.051471
2024-12-30 18:20:45,527 - INFO - [Train] Epoch 2, Batch 14: Loss=0.045504
2024-12-30 18:20:46,059 - INFO - [Train] Epoch 2, Batch 15: Loss=0.063386
2024-12-30 18:20:46,562 - INFO - [Train] Epoch 2, Batch 16: Loss=0.044131
2024-12-30 18:20:47,822 - INFO - [Train] Epoch 2, Batch 17: Loss=0.034425
2024-12-30 18:20:48,402 - INFO - [Train] Epoch 2, Batch 18: Loss=0.041767
2024-12-30 18:21:05,406 - INFO - [Train] Epoch 2, Batch 19: Loss=0.047570
2024-12-30 18:21:05,932 - INFO - [Train] Epoch 2, Batch 20: Loss=0.131217
2024-12-30 18:21:06,457 - INFO - [Train] Epoch 2, Batch 21: Loss=0.050118
2024-12-30 18:21:07,007 - INFO - [Train] Epoch 2, Batch 22: Loss=0.047234
2024-12-30 18:21:07,465 - INFO - [Train] Epoch 2, Batch 23: Loss=0.045319
2024-12-30 18:21:07,985 - INFO - [Train] Epoch 2, Batch 24: Loss=0.029141
2024-12-30 18:21:14,676 - INFO - [Train] Epoch 2, Batch 25: Loss=0.041338
2024-12-30 18:21:15,158 - INFO - [Train] Epoch 2, Batch 26: Loss=0.045977
2024-12-30 18:21:23,694 - INFO - [Train] Epoch 2, Batch 27: Loss=0.031798
2024-12-30 18:21:24,144 - INFO - [Train] Epoch 2, Batch 28: Loss=0.054694
2024-12-30 18:21:24,642 - INFO - [Train] Epoch 2, Batch 29: Loss=0.039368
2024-12-30 18:21:25,165 - INFO - [Train] Epoch 2, Batch 30: Loss=0.058684
2024-12-30 18:21:25,719 - INFO - [Train] Epoch 2, Batch 31: Loss=0.055243
2024-12-30 18:21:26,249 - INFO - [Train] Epoch 2, Batch 32: Loss=0.033705
2024-12-30 18:21:32,485 - INFO - [Train] Epoch 2, Batch 33: Loss=0.038985
2024-12-30 18:21:32,954 - INFO - [Train] Epoch 2, Batch 34: Loss=0.039547
2024-12-30 18:21:46,057 - INFO - [Train] Epoch 2, Batch 35: Loss=0.048002
2024-12-30 18:21:46,524 - INFO - [Train] Epoch 2, Batch 36: Loss=0.033035
2024-12-30 18:21:47,093 - INFO - [Train] Epoch 2, Batch 37: Loss=0.036910
2024-12-30 18:21:47,693 - INFO - [Train] Epoch 2, Batch 38: Loss=0.035459
2024-12-30 18:21:50,361 - INFO - [Train] Epoch 2, Batch 39: Loss=0.045374
2024-12-30 18:21:50,872 - INFO - [Train] Epoch 2, Batch 40: Loss=0.040036
2024-12-30 18:21:51,353 - INFO - [Train] Epoch 2, Batch 41: Loss=0.046957
2024-12-30 18:21:51,948 - INFO - [Train] Epoch 2, Batch 42: Loss=0.042298
2024-12-30 18:22:01,075 - INFO - [Train] Epoch 2, Batch 43: Loss=0.035042
2024-12-30 18:22:01,498 - INFO - [Train] Epoch 2, Batch 44: Loss=0.038036
2024-12-30 18:22:01,940 - INFO - [Train] Epoch 2, Batch 45: Loss=0.064097
2024-12-30 18:22:02,386 - INFO - [Train] Epoch 2, Batch 46: Loss=0.046825
2024-12-30 18:22:04,584 - INFO - [Train] Epoch 2, Batch 47: Loss=0.034527
2024-12-30 18:22:05,022 - INFO - [Train] Epoch 2, Batch 48: Loss=0.046147
2024-12-30 18:22:05,489 - INFO - [Train] Epoch 2, Batch 49: Loss=0.029034
2024-12-30 18:22:05,958 - INFO - [Train] Epoch 2, Batch 50: Loss=0.039569
2024-12-30 18:22:08,243 - INFO - [Train] Epoch 2 completed -> Avg Loss: 0.047313
2024-12-30 18:22:08,243 - INFO - [Train] Epoch 2 completed -> Avg Loss: 0.047313
2024-12-30 18:22:08,273 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_2.pth
2024-12-30 18:22:08,274 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_2.pth
2024-12-30 18:22:08,274 - INFO - [Train] Epoch 3/10 started.
2024-12-30 18:22:46,886 - INFO - [Train] Epoch 3, Batch 1: Loss=0.028968
2024-12-30 18:22:47,450 - INFO - [Train] Epoch 3, Batch 2: Loss=0.033579
2024-12-30 18:22:47,997 - INFO - [Train] Epoch 3, Batch 3: Loss=0.071061
2024-12-30 18:22:55,072 - INFO - [Train] Epoch 3, Batch 4: Loss=0.039015
2024-12-30 18:22:55,634 - INFO - [Train] Epoch 3, Batch 5: Loss=0.051741
2024-12-30 18:22:56,153 - INFO - [Train] Epoch 3, Batch 6: Loss=0.029010
2024-12-30 18:22:56,637 - INFO - [Train] Epoch 3, Batch 7: Loss=0.050933
2024-12-30 18:22:57,088 - INFO - [Train] Epoch 3, Batch 8: Loss=0.049155
2024-12-30 18:23:07,428 - INFO - [Train] Epoch 3, Batch 9: Loss=0.042107
2024-12-30 18:23:15,603 - INFO - [Train] Epoch 3, Batch 10: Loss=0.038626
2024-12-30 18:23:16,067 - INFO - [Train] Epoch 3, Batch 11: Loss=0.035134
2024-12-30 18:23:18,459 - INFO - [Train] Epoch 3, Batch 12: Loss=0.048451
2024-12-30 18:23:18,907 - INFO - [Train] Epoch 3, Batch 13: Loss=0.042918
2024-12-30 18:23:19,358 - INFO - [Train] Epoch 3, Batch 14: Loss=0.047227
2024-12-30 18:23:19,845 - INFO - [Train] Epoch 3, Batch 15: Loss=0.051958
2024-12-30 18:23:22,923 - INFO - [Train] Epoch 3, Batch 16: Loss=0.032286
2024-12-30 18:23:29,535 - INFO - [Train] Epoch 3, Batch 17: Loss=0.036348
2024-12-30 18:23:29,992 - INFO - [Train] Epoch 3, Batch 18: Loss=0.049897
2024-12-30 18:23:30,513 - INFO - [Train] Epoch 3, Batch 19: Loss=0.050690
2024-12-30 18:23:36,194 - INFO - [Train] Epoch 3, Batch 20: Loss=0.033205
2024-12-30 18:23:36,694 - INFO - [Train] Epoch 3, Batch 21: Loss=0.039292
2024-12-30 18:23:37,174 - INFO - [Train] Epoch 3, Batch 22: Loss=0.034790
2024-12-30 18:23:37,683 - INFO - [Train] Epoch 3, Batch 23: Loss=0.029676
2024-12-30 18:23:42,611 - INFO - [Train] Epoch 3, Batch 24: Loss=0.036033
2024-12-30 18:23:47,599 - INFO - [Train] Epoch 3, Batch 25: Loss=0.034995
2024-12-30 18:23:48,063 - INFO - [Train] Epoch 3, Batch 26: Loss=0.044324
2024-12-30 18:23:48,518 - INFO - [Train] Epoch 3, Batch 27: Loss=0.039609
2024-12-30 18:23:54,861 - INFO - [Train] Epoch 3, Batch 28: Loss=0.037571
2024-12-30 18:23:55,325 - INFO - [Train] Epoch 3, Batch 29: Loss=0.044873
2024-12-30 18:23:55,766 - INFO - [Train] Epoch 3, Batch 30: Loss=0.042140
2024-12-30 18:23:56,198 - INFO - [Train] Epoch 3, Batch 31: Loss=0.032355
2024-12-30 18:24:07,622 - INFO - [Train] Epoch 3, Batch 32: Loss=0.043174
2024-12-30 18:24:09,895 - INFO - [Train] Epoch 3, Batch 33: Loss=0.051232
2024-12-30 18:24:10,332 - INFO - [Train] Epoch 3, Batch 34: Loss=0.040768
2024-12-30 18:24:10,826 - INFO - [Train] Epoch 3, Batch 35: Loss=0.041208
2024-12-30 18:24:17,374 - INFO - [Train] Epoch 3, Batch 36: Loss=0.038907
2024-12-30 18:24:17,836 - INFO - [Train] Epoch 3, Batch 37: Loss=0.040828
2024-12-30 18:24:18,282 - INFO - [Train] Epoch 3, Batch 38: Loss=0.151976
2024-12-30 18:24:21,147 - INFO - [Train] Epoch 3, Batch 39: Loss=0.040062
2024-12-30 18:24:25,041 - INFO - [Train] Epoch 3, Batch 40: Loss=0.033544
2024-12-30 18:24:28,909 - INFO - [Train] Epoch 3, Batch 41: Loss=0.037856
2024-12-30 18:24:29,363 - INFO - [Train] Epoch 3, Batch 42: Loss=0.028874
2024-12-30 18:24:29,840 - INFO - [Train] Epoch 3, Batch 43: Loss=0.037095
2024-12-30 18:24:30,304 - INFO - [Train] Epoch 3, Batch 44: Loss=0.041546
2024-12-30 18:24:30,808 - INFO - [Train] Epoch 3, Batch 45: Loss=0.052817
2024-12-30 18:24:31,267 - INFO - [Train] Epoch 3, Batch 46: Loss=0.045821
2024-12-30 18:24:31,705 - INFO - [Train] Epoch 3, Batch 47: Loss=0.117682
2024-12-30 18:24:34,024 - INFO - [Train] Epoch 3, Batch 48: Loss=0.042020
2024-12-30 18:24:45,043 - INFO - [Train] Epoch 3, Batch 49: Loss=0.037772
2024-12-30 18:24:45,470 - INFO - [Train] Epoch 3, Batch 50: Loss=0.040503
2024-12-30 18:24:48,072 - INFO - [Train] Epoch 3 completed -> Avg Loss: 0.044833
2024-12-30 18:24:48,072 - INFO - [Train] Epoch 3 completed -> Avg Loss: 0.044833
2024-12-30 18:24:48,101 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_3.pth
2024-12-30 18:24:48,101 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_3.pth
2024-12-30 18:24:48,101 - INFO - [Train] Epoch 4/10 started.
2024-12-30 18:25:14,158 - INFO - [Train] Epoch 4, Batch 1: Loss=0.044976
2024-12-30 18:25:18,063 - INFO - [Train] Epoch 4, Batch 2: Loss=0.126235
2024-12-30 18:25:18,522 - INFO - [Train] Epoch 4, Batch 3: Loss=0.033491
2024-12-30 18:25:23,067 - INFO - [Train] Epoch 4, Batch 4: Loss=0.038266
2024-12-30 18:25:23,526 - INFO - [Train] Epoch 4, Batch 5: Loss=0.040248
2024-12-30 18:25:23,981 - INFO - [Train] Epoch 4, Batch 6: Loss=0.044208
2024-12-30 18:25:24,427 - INFO - [Train] Epoch 4, Batch 7: Loss=0.038437
2024-12-30 18:25:24,961 - INFO - [Train] Epoch 4, Batch 8: Loss=0.045750
2024-12-30 18:25:31,221 - INFO - [Train] Epoch 4, Batch 9: Loss=0.040240
2024-12-30 18:25:46,526 - INFO - [Train] Epoch 4, Batch 10: Loss=0.031264
2024-12-30 18:25:47,141 - INFO - [Train] Epoch 4, Batch 11: Loss=0.041681
2024-12-30 18:25:47,701 - INFO - [Train] Epoch 4, Batch 12: Loss=0.042018
2024-12-30 18:25:48,279 - INFO - [Train] Epoch 4, Batch 13: Loss=0.043357
2024-12-30 18:25:48,767 - INFO - [Train] Epoch 4, Batch 14: Loss=0.037133
2024-12-30 18:25:49,295 - INFO - [Train] Epoch 4, Batch 15: Loss=0.040356
2024-12-30 18:25:49,824 - INFO - [Train] Epoch 4, Batch 16: Loss=0.030993
2024-12-30 18:25:50,255 - INFO - [Train] Epoch 4, Batch 17: Loss=0.039740
2024-12-30 18:26:12,346 - INFO - [Train] Epoch 4, Batch 18: Loss=0.039249
2024-12-30 18:26:12,924 - INFO - [Train] Epoch 4, Batch 19: Loss=0.037393
2024-12-30 18:26:13,424 - INFO - [Train] Epoch 4, Batch 20: Loss=0.039747
2024-12-30 18:26:13,945 - INFO - [Train] Epoch 4, Batch 21: Loss=0.032873
2024-12-30 18:26:14,397 - INFO - [Train] Epoch 4, Batch 22: Loss=0.040822
2024-12-30 18:26:14,851 - INFO - [Train] Epoch 4, Batch 23: Loss=0.040474
2024-12-30 18:26:15,369 - INFO - [Train] Epoch 4, Batch 24: Loss=0.038848
2024-12-30 18:26:15,830 - INFO - [Train] Epoch 4, Batch 25: Loss=0.047373
2024-12-30 18:26:21,237 - INFO - [Train] Epoch 4, Batch 26: Loss=0.042499
2024-12-30 18:26:21,755 - INFO - [Train] Epoch 4, Batch 27: Loss=0.036738
2024-12-30 18:26:22,289 - INFO - [Train] Epoch 4, Batch 28: Loss=0.060549
2024-12-30 18:26:22,775 - INFO - [Train] Epoch 4, Batch 29: Loss=0.035720
2024-12-30 18:26:25,403 - INFO - [Train] Epoch 4, Batch 30: Loss=0.041035
2024-12-30 18:26:25,854 - INFO - [Train] Epoch 4, Batch 31: Loss=0.037757
2024-12-30 18:26:26,302 - INFO - [Train] Epoch 4, Batch 32: Loss=0.028690
2024-12-30 18:26:26,743 - INFO - [Train] Epoch 4, Batch 33: Loss=0.040592
2024-12-30 18:26:36,302 - INFO - [Train] Epoch 4, Batch 34: Loss=0.031183
2024-12-30 18:26:36,757 - INFO - [Train] Epoch 4, Batch 35: Loss=0.037842
2024-12-30 18:26:47,403 - INFO - [Train] Epoch 4, Batch 36: Loss=0.038438
2024-12-30 18:26:47,997 - INFO - [Train] Epoch 4, Batch 37: Loss=0.031373
2024-12-30 18:26:48,465 - INFO - [Train] Epoch 4, Batch 38: Loss=0.036858
2024-12-30 18:26:49,051 - INFO - [Train] Epoch 4, Batch 39: Loss=0.035218
2024-12-30 18:26:49,655 - INFO - [Train] Epoch 4, Batch 40: Loss=0.039133
2024-12-30 18:26:50,160 - INFO - [Train] Epoch 4, Batch 41: Loss=0.030387
2024-12-30 18:26:55,847 - INFO - [Train] Epoch 4, Batch 42: Loss=0.038952
2024-12-30 18:26:56,357 - INFO - [Train] Epoch 4, Batch 43: Loss=0.033739
2024-12-30 18:26:59,525 - INFO - [Train] Epoch 4, Batch 44: Loss=0.123058
2024-12-30 18:26:59,993 - INFO - [Train] Epoch 4, Batch 45: Loss=0.037114
2024-12-30 18:27:07,198 - INFO - [Train] Epoch 4, Batch 46: Loss=0.028637
2024-12-30 18:27:07,660 - INFO - [Train] Epoch 4, Batch 47: Loss=0.037739
2024-12-30 18:27:08,123 - INFO - [Train] Epoch 4, Batch 48: Loss=0.028610
2024-12-30 18:27:08,582 - INFO - [Train] Epoch 4, Batch 49: Loss=0.061500
2024-12-30 18:27:10,884 - INFO - [Train] Epoch 4, Batch 50: Loss=0.039983
2024-12-30 18:27:12,430 - INFO - [Train] Epoch 4 completed -> Avg Loss: 0.042170
2024-12-30 18:27:12,430 - INFO - [Train] Epoch 4 completed -> Avg Loss: 0.042170
2024-12-30 18:27:12,454 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_4.pth
2024-12-30 18:27:12,454 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_4.pth
2024-12-30 18:27:12,455 - INFO - [Train] Epoch 5/10 started.
2024-12-30 18:27:44,035 - INFO - [Train] Epoch 5, Batch 1: Loss=0.068279
2024-12-30 18:27:44,552 - INFO - [Train] Epoch 5, Batch 2: Loss=0.043053
2024-12-30 18:27:45,147 - INFO - [Train] Epoch 5, Batch 3: Loss=0.033833
2024-12-30 18:27:45,694 - INFO - [Train] Epoch 5, Batch 4: Loss=0.030041
2024-12-30 18:27:46,282 - INFO - [Train] Epoch 5, Batch 5: Loss=0.040209
2024-12-30 18:27:46,776 - INFO - [Train] Epoch 5, Batch 6: Loss=0.043935
2024-12-30 18:27:47,272 - INFO - [Train] Epoch 5, Batch 7: Loss=0.034210
2024-12-30 18:27:47,786 - INFO - [Train] Epoch 5, Batch 8: Loss=0.038672
2024-12-30 18:28:06,765 - INFO - [Train] Epoch 5, Batch 9: Loss=0.047620
2024-12-30 18:28:07,325 - INFO - [Train] Epoch 5, Batch 10: Loss=0.035769
2024-12-30 18:28:07,945 - INFO - [Train] Epoch 5, Batch 11: Loss=0.029229
2024-12-30 18:28:08,465 - INFO - [Train] Epoch 5, Batch 12: Loss=0.039059
2024-12-30 18:28:09,104 - INFO - [Train] Epoch 5, Batch 13: Loss=0.034522
2024-12-30 18:28:09,605 - INFO - [Train] Epoch 5, Batch 14: Loss=0.031926
2024-12-30 18:28:10,183 - INFO - [Train] Epoch 5, Batch 15: Loss=0.037635
2024-12-30 18:28:10,743 - INFO - [Train] Epoch 5, Batch 16: Loss=0.043909
2024-12-30 18:28:30,970 - INFO - [Train] Epoch 5, Batch 17: Loss=0.039372
2024-12-30 18:28:31,434 - INFO - [Train] Epoch 5, Batch 18: Loss=0.035982
2024-12-30 18:28:31,912 - INFO - [Train] Epoch 5, Batch 19: Loss=0.038132
2024-12-30 18:28:32,369 - INFO - [Train] Epoch 5, Batch 20: Loss=0.032181
2024-12-30 18:28:32,826 - INFO - [Train] Epoch 5, Batch 21: Loss=0.053353
2024-12-30 18:28:33,359 - INFO - [Train] Epoch 5, Batch 22: Loss=0.142182
2024-12-30 18:28:35,022 - INFO - [Train] Epoch 5, Batch 23: Loss=0.030923
2024-12-30 18:28:35,728 - INFO - [Train] Epoch 5, Batch 24: Loss=0.126301
2024-12-30 18:28:59,724 - INFO - [Train] Epoch 5, Batch 25: Loss=0.034924
2024-12-30 18:29:00,264 - INFO - [Train] Epoch 5, Batch 26: Loss=0.028983
2024-12-30 18:29:00,743 - INFO - [Train] Epoch 5, Batch 27: Loss=0.036708
2024-12-30 18:29:01,191 - INFO - [Train] Epoch 5, Batch 28: Loss=0.028370
2024-12-30 18:29:01,662 - INFO - [Train] Epoch 5, Batch 29: Loss=0.033720
2024-12-30 18:29:02,116 - INFO - [Train] Epoch 5, Batch 30: Loss=0.039149
2024-12-30 18:29:02,689 - INFO - [Train] Epoch 5, Batch 31: Loss=0.032670
2024-12-30 18:29:03,115 - INFO - [Train] Epoch 5, Batch 32: Loss=0.035132
2024-12-30 18:29:26,807 - INFO - [Train] Epoch 5, Batch 33: Loss=0.032782
2024-12-30 18:29:27,300 - INFO - [Train] Epoch 5, Batch 34: Loss=0.039847
2024-12-30 18:29:27,838 - INFO - [Train] Epoch 5, Batch 35: Loss=0.028499
2024-12-30 18:29:28,349 - INFO - [Train] Epoch 5, Batch 36: Loss=0.042959
2024-12-30 18:29:28,858 - INFO - [Train] Epoch 5, Batch 37: Loss=0.035700
2024-12-30 18:29:29,376 - INFO - [Train] Epoch 5, Batch 38: Loss=0.035479
2024-12-30 18:29:29,863 - INFO - [Train] Epoch 5, Batch 39: Loss=0.035406
2024-12-30 18:29:30,365 - INFO - [Train] Epoch 5, Batch 40: Loss=0.028472
2024-12-30 18:29:43,025 - INFO - [Train] Epoch 5, Batch 41: Loss=0.040889
2024-12-30 18:29:43,508 - INFO - [Train] Epoch 5, Batch 42: Loss=0.029745
2024-12-30 18:29:43,946 - INFO - [Train] Epoch 5, Batch 43: Loss=0.033815
2024-12-30 18:29:44,378 - INFO - [Train] Epoch 5, Batch 44: Loss=0.037315
2024-12-30 18:29:44,803 - INFO - [Train] Epoch 5, Batch 45: Loss=0.044213
2024-12-30 18:29:45,227 - INFO - [Train] Epoch 5, Batch 46: Loss=0.038950
2024-12-30 18:29:45,712 - INFO - [Train] Epoch 5, Batch 47: Loss=0.036613
2024-12-30 18:29:46,185 - INFO - [Train] Epoch 5, Batch 48: Loss=0.041834
2024-12-30 18:29:55,922 - INFO - [Train] Epoch 5, Batch 49: Loss=0.032633
2024-12-30 18:29:56,392 - INFO - [Train] Epoch 5, Batch 50: Loss=0.039004
2024-12-30 18:29:58,292 - INFO - [Train] Epoch 5 completed -> Avg Loss: 0.041083
2024-12-30 18:29:58,292 - INFO - [Train] Epoch 5 completed -> Avg Loss: 0.041083
2024-12-30 18:29:58,310 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_5.pth
2024-12-30 18:29:58,310 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_5.pth
2024-12-30 18:29:58,311 - INFO - [Train] Epoch 6/10 started.
2024-12-30 18:30:33,217 - INFO - [Train] Epoch 6, Batch 1: Loss=0.029674
2024-12-30 18:30:33,750 - INFO - [Train] Epoch 6, Batch 2: Loss=0.040816
2024-12-30 18:30:34,282 - INFO - [Train] Epoch 6, Batch 3: Loss=0.036407
2024-12-30 18:30:37,383 - INFO - [Train] Epoch 6, Batch 4: Loss=0.032548
2024-12-30 18:30:37,842 - INFO - [Train] Epoch 6, Batch 5: Loss=0.029967
2024-12-30 18:30:38,303 - INFO - [Train] Epoch 6, Batch 6: Loss=0.038461
2024-12-30 18:30:38,749 - INFO - [Train] Epoch 6, Batch 7: Loss=0.043969
2024-12-30 18:30:39,208 - INFO - [Train] Epoch 6, Batch 8: Loss=0.030129
2024-12-30 18:30:54,197 - INFO - [Train] Epoch 6, Batch 9: Loss=0.042321
2024-12-30 18:30:54,738 - INFO - [Train] Epoch 6, Batch 10: Loss=0.034661
2024-12-30 18:30:55,260 - INFO - [Train] Epoch 6, Batch 11: Loss=0.037433
2024-12-30 18:30:55,776 - INFO - [Train] Epoch 6, Batch 12: Loss=0.045711
2024-12-30 18:30:56,297 - INFO - [Train] Epoch 6, Batch 13: Loss=0.035625
2024-12-30 18:30:56,830 - INFO - [Train] Epoch 6, Batch 14: Loss=0.032081
2024-12-30 18:30:57,364 - INFO - [Train] Epoch 6, Batch 15: Loss=0.040711
2024-12-30 18:30:57,885 - INFO - [Train] Epoch 6, Batch 16: Loss=0.044526
2024-12-30 18:31:18,878 - INFO - [Train] Epoch 6, Batch 17: Loss=0.031867
2024-12-30 18:31:19,399 - INFO - [Train] Epoch 6, Batch 18: Loss=0.038190
2024-12-30 18:31:19,952 - INFO - [Train] Epoch 6, Batch 19: Loss=0.044403
2024-12-30 18:31:20,430 - INFO - [Train] Epoch 6, Batch 20: Loss=0.031463
2024-12-30 18:31:20,896 - INFO - [Train] Epoch 6, Batch 21: Loss=0.035297
2024-12-30 18:31:21,417 - INFO - [Train] Epoch 6, Batch 22: Loss=0.035485
2024-12-30 18:31:21,943 - INFO - [Train] Epoch 6, Batch 23: Loss=0.031923
2024-12-30 18:31:22,429 - INFO - [Train] Epoch 6, Batch 24: Loss=0.033840
2024-12-30 18:31:36,643 - INFO - [Train] Epoch 6, Batch 25: Loss=0.036069
2024-12-30 18:31:37,244 - INFO - [Train] Epoch 6, Batch 26: Loss=0.123749
2024-12-30 18:31:37,836 - INFO - [Train] Epoch 6, Batch 27: Loss=0.028312
2024-12-30 18:31:49,896 - INFO - [Train] Epoch 6, Batch 28: Loss=0.036483
2024-12-30 18:31:50,445 - INFO - [Train] Epoch 6, Batch 29: Loss=0.028312
2024-12-30 18:31:50,977 - INFO - [Train] Epoch 6, Batch 30: Loss=0.067482
2024-12-30 18:31:51,521 - INFO - [Train] Epoch 6, Batch 31: Loss=0.038425
2024-12-30 18:31:52,073 - INFO - [Train] Epoch 6, Batch 32: Loss=0.136776
2024-12-30 18:31:52,576 - INFO - [Train] Epoch 6, Batch 33: Loss=0.037543
2024-12-30 18:31:53,183 - INFO - [Train] Epoch 6, Batch 34: Loss=0.031634
2024-12-30 18:31:53,731 - INFO - [Train] Epoch 6, Batch 35: Loss=0.032266
2024-12-30 18:32:11,589 - INFO - [Train] Epoch 6, Batch 36: Loss=0.031409
2024-12-30 18:32:12,134 - INFO - [Train] Epoch 6, Batch 37: Loss=0.039304
2024-12-30 18:32:12,668 - INFO - [Train] Epoch 6, Batch 38: Loss=0.041266
2024-12-30 18:32:13,212 - INFO - [Train] Epoch 6, Batch 39: Loss=0.032896
2024-12-30 18:32:13,652 - INFO - [Train] Epoch 6, Batch 40: Loss=0.033324
2024-12-30 18:32:14,980 - INFO - [Train] Epoch 6, Batch 41: Loss=0.034931
2024-12-30 18:32:15,506 - INFO - [Train] Epoch 6, Batch 42: Loss=0.037177
2024-12-30 18:32:16,066 - INFO - [Train] Epoch 6, Batch 43: Loss=0.029340
2024-12-30 18:32:22,164 - INFO - [Train] Epoch 6, Batch 44: Loss=0.031453
2024-12-30 18:32:22,624 - INFO - [Train] Epoch 6, Batch 45: Loss=0.037322
2024-12-30 18:32:23,052 - INFO - [Train] Epoch 6, Batch 46: Loss=0.038674
2024-12-30 18:32:23,502 - INFO - [Train] Epoch 6, Batch 47: Loss=0.033623
2024-12-30 18:32:23,962 - INFO - [Train] Epoch 6, Batch 48: Loss=0.029372
2024-12-30 18:32:27,267 - INFO - [Train] Epoch 6, Batch 49: Loss=0.034984
2024-12-30 18:32:27,711 - INFO - [Train] Epoch 6, Batch 50: Loss=0.037897
2024-12-30 18:32:29,139 - INFO - [Train] Epoch 6 completed -> Avg Loss: 0.039951
2024-12-30 18:32:29,139 - INFO - [Train] Epoch 6 completed -> Avg Loss: 0.039951
2024-12-30 18:32:29,156 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_6.pth
2024-12-30 18:32:29,156 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_6.pth
2024-12-30 18:32:29,157 - INFO - [Train] Epoch 7/10 started.
2024-12-30 18:33:02,316 - INFO - [Train] Epoch 7, Batch 1: Loss=0.029762
2024-12-30 18:33:02,843 - INFO - [Train] Epoch 7, Batch 2: Loss=0.036082
2024-12-30 18:33:03,367 - INFO - [Train] Epoch 7, Batch 3: Loss=0.035380
2024-12-30 18:33:03,888 - INFO - [Train] Epoch 7, Batch 4: Loss=0.031054
2024-12-30 18:33:04,393 - INFO - [Train] Epoch 7, Batch 5: Loss=0.030619
2024-12-30 18:33:05,457 - INFO - [Train] Epoch 7, Batch 6: Loss=0.034627
2024-12-30 18:33:05,983 - INFO - [Train] Epoch 7, Batch 7: Loss=0.036479
2024-12-30 18:33:06,515 - INFO - [Train] Epoch 7, Batch 8: Loss=0.033300
2024-12-30 18:33:24,627 - INFO - [Train] Epoch 7, Batch 9: Loss=0.045136
2024-12-30 18:33:25,114 - INFO - [Train] Epoch 7, Batch 10: Loss=0.029251
2024-12-30 18:33:25,573 - INFO - [Train] Epoch 7, Batch 11: Loss=0.029953
2024-12-30 18:33:26,034 - INFO - [Train] Epoch 7, Batch 12: Loss=0.040016
2024-12-30 18:33:26,494 - INFO - [Train] Epoch 7, Batch 13: Loss=0.038879
2024-12-30 18:33:27,767 - INFO - [Train] Epoch 7, Batch 14: Loss=0.038052
2024-12-30 18:33:28,253 - INFO - [Train] Epoch 7, Batch 15: Loss=0.030410
2024-12-30 18:33:28,788 - INFO - [Train] Epoch 7, Batch 16: Loss=0.039833
2024-12-30 18:33:43,353 - INFO - [Train] Epoch 7, Batch 17: Loss=0.028154
2024-12-30 18:33:43,813 - INFO - [Train] Epoch 7, Batch 18: Loss=0.027950
2024-12-30 18:33:44,296 - INFO - [Train] Epoch 7, Batch 19: Loss=0.032914
2024-12-30 18:33:44,843 - INFO - [Train] Epoch 7, Batch 20: Loss=0.032667
2024-12-30 18:33:45,384 - INFO - [Train] Epoch 7, Batch 21: Loss=0.030075
2024-12-30 18:33:45,874 - INFO - [Train] Epoch 7, Batch 22: Loss=0.038210
2024-12-30 18:33:46,415 - INFO - [Train] Epoch 7, Batch 23: Loss=0.032590
2024-12-30 18:33:46,967 - INFO - [Train] Epoch 7, Batch 24: Loss=0.034829
2024-12-30 18:34:04,724 - INFO - [Train] Epoch 7, Batch 25: Loss=0.037645
2024-12-30 18:34:05,283 - INFO - [Train] Epoch 7, Batch 26: Loss=0.029838
2024-12-30 18:34:05,775 - INFO - [Train] Epoch 7, Batch 27: Loss=0.028125
2024-12-30 18:34:06,252 - INFO - [Train] Epoch 7, Batch 28: Loss=0.032580
2024-12-30 18:34:06,723 - INFO - [Train] Epoch 7, Batch 29: Loss=0.034139
2024-12-30 18:34:07,241 - INFO - [Train] Epoch 7, Batch 30: Loss=0.065345
2024-12-30 18:34:07,692 - INFO - [Train] Epoch 7, Batch 31: Loss=0.035209
2024-12-30 18:34:08,184 - INFO - [Train] Epoch 7, Batch 32: Loss=0.031565
2024-12-30 18:34:18,479 - INFO - [Train] Epoch 7, Batch 33: Loss=0.034591
2024-12-30 18:34:18,973 - INFO - [Train] Epoch 7, Batch 34: Loss=0.030399
2024-12-30 18:34:37,240 - INFO - [Train] Epoch 7, Batch 35: Loss=0.035147
2024-12-30 18:34:37,729 - INFO - [Train] Epoch 7, Batch 36: Loss=0.036479
2024-12-30 18:34:38,205 - INFO - [Train] Epoch 7, Batch 37: Loss=0.040569
2024-12-30 18:34:38,678 - INFO - [Train] Epoch 7, Batch 38: Loss=0.031786
2024-12-30 18:34:39,121 - INFO - [Train] Epoch 7, Batch 39: Loss=0.035674
2024-12-30 18:34:39,592 - INFO - [Train] Epoch 7, Batch 40: Loss=0.040282
2024-12-30 18:34:40,026 - INFO - [Train] Epoch 7, Batch 41: Loss=0.037080
2024-12-30 18:34:40,451 - INFO - [Train] Epoch 7, Batch 42: Loss=0.133508
2024-12-30 18:34:49,339 - INFO - [Train] Epoch 7, Batch 43: Loss=0.040724
2024-12-30 18:34:49,766 - INFO - [Train] Epoch 7, Batch 44: Loss=0.029014
2024-12-30 18:34:50,191 - INFO - [Train] Epoch 7, Batch 45: Loss=0.038164
2024-12-30 18:34:50,617 - INFO - [Train] Epoch 7, Batch 46: Loss=0.028048
2024-12-30 18:34:51,039 - INFO - [Train] Epoch 7, Batch 47: Loss=0.155269
2024-12-30 18:34:51,459 - INFO - [Train] Epoch 7, Batch 48: Loss=0.040379
2024-12-30 18:34:51,882 - INFO - [Train] Epoch 7, Batch 49: Loss=0.035244
2024-12-30 18:34:52,303 - INFO - [Train] Epoch 7, Batch 50: Loss=0.028406
2024-12-30 18:34:53,993 - INFO - [Train] Epoch 7 completed -> Avg Loss: 0.039229
2024-12-30 18:34:53,993 - INFO - [Train] Epoch 7 completed -> Avg Loss: 0.039229
2024-12-30 18:34:54,015 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_7.pth
2024-12-30 18:34:54,015 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_7.pth
2024-12-30 18:34:54,016 - INFO - [Train] Epoch 8/10 started.
2024-12-30 18:35:27,601 - INFO - [Train] Epoch 8, Batch 1: Loss=0.034845
2024-12-30 18:35:37,973 - INFO - [Train] Epoch 8, Batch 2: Loss=0.037535
2024-12-30 18:35:49,328 - INFO - [Train] Epoch 8, Batch 3: Loss=0.030734
2024-12-30 18:35:49,795 - INFO - [Train] Epoch 8, Batch 4: Loss=0.033950
2024-12-30 18:35:50,265 - INFO - [Train] Epoch 8, Batch 5: Loss=0.036308
2024-12-30 18:35:50,819 - INFO - [Train] Epoch 8, Batch 6: Loss=0.029694
2024-12-30 18:35:51,414 - INFO - [Train] Epoch 8, Batch 7: Loss=0.031274
2024-12-30 18:35:51,989 - INFO - [Train] Epoch 8, Batch 8: Loss=0.029808
2024-12-30 18:35:52,493 - INFO - [Train] Epoch 8, Batch 9: Loss=0.040571
2024-12-30 18:36:00,023 - INFO - [Train] Epoch 8, Batch 10: Loss=0.030989
2024-12-30 18:36:09,535 - INFO - [Train] Epoch 8, Batch 11: Loss=0.034723
2024-12-30 18:36:10,057 - INFO - [Train] Epoch 8, Batch 12: Loss=0.036579
2024-12-30 18:36:10,566 - INFO - [Train] Epoch 8, Batch 13: Loss=0.032657
2024-12-30 18:36:11,133 - INFO - [Train] Epoch 8, Batch 14: Loss=0.030839
2024-12-30 18:36:11,670 - INFO - [Train] Epoch 8, Batch 15: Loss=0.031630
2024-12-30 18:36:12,182 - INFO - [Train] Epoch 8, Batch 16: Loss=0.031517
2024-12-30 18:36:12,693 - INFO - [Train] Epoch 8, Batch 17: Loss=0.032288
2024-12-30 18:36:19,369 - INFO - [Train] Epoch 8, Batch 18: Loss=0.027963
2024-12-30 18:36:33,536 - INFO - [Train] Epoch 8, Batch 19: Loss=0.031748
2024-12-30 18:36:34,074 - INFO - [Train] Epoch 8, Batch 20: Loss=0.027937
2024-12-30 18:36:34,647 - INFO - [Train] Epoch 8, Batch 21: Loss=0.134624
2024-12-30 18:36:35,211 - INFO - [Train] Epoch 8, Batch 22: Loss=0.040386
2024-12-30 18:36:35,756 - INFO - [Train] Epoch 8, Batch 23: Loss=0.037775
2024-12-30 18:36:36,275 - INFO - [Train] Epoch 8, Batch 24: Loss=0.030883
2024-12-30 18:36:36,776 - INFO - [Train] Epoch 8, Batch 25: Loss=0.033118
2024-12-30 18:36:37,278 - INFO - [Train] Epoch 8, Batch 26: Loss=0.065597
2024-12-30 18:36:47,987 - INFO - [Train] Epoch 8, Batch 27: Loss=0.035951
2024-12-30 18:36:52,871 - INFO - [Train] Epoch 8, Batch 28: Loss=0.032421
2024-12-30 18:36:53,508 - INFO - [Train] Epoch 8, Batch 29: Loss=0.030758
2024-12-30 18:36:54,117 - INFO - [Train] Epoch 8, Batch 30: Loss=0.032501
2024-12-30 18:36:54,613 - INFO - [Train] Epoch 8, Batch 31: Loss=0.030134
2024-12-30 18:36:55,107 - INFO - [Train] Epoch 8, Batch 32: Loss=0.039866
2024-12-30 18:36:55,691 - INFO - [Train] Epoch 8, Batch 33: Loss=0.033800
2024-12-30 18:36:56,223 - INFO - [Train] Epoch 8, Batch 34: Loss=0.036167
2024-12-30 18:37:08,788 - INFO - [Train] Epoch 8, Batch 35: Loss=0.041087
2024-12-30 18:37:15,504 - INFO - [Train] Epoch 8, Batch 36: Loss=0.037793
2024-12-30 18:37:15,999 - INFO - [Train] Epoch 8, Batch 37: Loss=0.028448
2024-12-30 18:37:16,495 - INFO - [Train] Epoch 8, Batch 38: Loss=0.027858
2024-12-30 18:37:16,985 - INFO - [Train] Epoch 8, Batch 39: Loss=0.030722
2024-12-30 18:37:17,527 - INFO - [Train] Epoch 8, Batch 40: Loss=0.034675
2024-12-30 18:37:18,037 - INFO - [Train] Epoch 8, Batch 41: Loss=0.027841
2024-12-30 18:37:18,560 - INFO - [Train] Epoch 8, Batch 42: Loss=0.039095
2024-12-30 18:37:21,867 - INFO - [Train] Epoch 8, Batch 43: Loss=0.136356
2024-12-30 18:37:30,442 - INFO - [Train] Epoch 8, Batch 44: Loss=0.032353
2024-12-30 18:37:30,916 - INFO - [Train] Epoch 8, Batch 45: Loss=0.040728
2024-12-30 18:37:31,374 - INFO - [Train] Epoch 8, Batch 46: Loss=0.032827
2024-12-30 18:37:31,818 - INFO - [Train] Epoch 8, Batch 47: Loss=0.040103
2024-12-30 18:37:32,238 - INFO - [Train] Epoch 8, Batch 48: Loss=0.030493
2024-12-30 18:37:32,659 - INFO - [Train] Epoch 8, Batch 49: Loss=0.031333
2024-12-30 18:37:33,081 - INFO - [Train] Epoch 8, Batch 50: Loss=0.035214
2024-12-30 18:37:35,113 - INFO - [Train] Epoch 8 completed -> Avg Loss: 0.038290
2024-12-30 18:37:35,114 - INFO - [Train] Epoch 8 completed -> Avg Loss: 0.038290
2024-12-30 18:37:35,128 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_8.pth
2024-12-30 18:37:35,128 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_8.pth
2024-12-30 18:37:35,129 - INFO - [Train] Epoch 9/10 started.
2024-12-30 18:38:15,218 - INFO - [Train] Epoch 9, Batch 1: Loss=0.031788
2024-12-30 18:38:15,796 - INFO - [Train] Epoch 9, Batch 2: Loss=0.040537
2024-12-30 18:38:16,373 - INFO - [Train] Epoch 9, Batch 3: Loss=0.027980
2024-12-30 18:38:16,946 - INFO - [Train] Epoch 9, Batch 4: Loss=0.034206
2024-12-30 18:38:17,561 - INFO - [Train] Epoch 9, Batch 5: Loss=0.032116
2024-12-30 18:38:18,233 - INFO - [Train] Epoch 9, Batch 6: Loss=0.038235
2024-12-30 18:38:18,808 - INFO - [Train] Epoch 9, Batch 7: Loss=0.032084
2024-12-30 18:38:19,371 - INFO - [Train] Epoch 9, Batch 8: Loss=0.034408
2024-12-30 18:38:37,048 - INFO - [Train] Epoch 9, Batch 9: Loss=0.027778
2024-12-30 18:38:37,549 - INFO - [Train] Epoch 9, Batch 10: Loss=0.027777
2024-12-30 18:38:38,051 - INFO - [Train] Epoch 9, Batch 11: Loss=0.029740
2024-12-30 18:38:38,530 - INFO - [Train] Epoch 9, Batch 12: Loss=0.037462
2024-12-30 18:38:39,015 - INFO - [Train] Epoch 9, Batch 13: Loss=0.030342
2024-12-30 18:38:39,486 - INFO - [Train] Epoch 9, Batch 14: Loss=0.032283
2024-12-30 18:38:39,963 - INFO - [Train] Epoch 9, Batch 15: Loss=0.027750
2024-12-30 18:38:40,445 - INFO - [Train] Epoch 9, Batch 16: Loss=0.034666
2024-12-30 18:38:56,601 - INFO - [Train] Epoch 9, Batch 17: Loss=0.030034
2024-12-30 18:38:57,229 - INFO - [Train] Epoch 9, Batch 18: Loss=0.027645
2024-12-30 18:38:57,826 - INFO - [Train] Epoch 9, Batch 19: Loss=0.041209
2024-12-30 18:38:58,364 - INFO - [Train] Epoch 9, Batch 20: Loss=0.032010
2024-12-30 18:38:58,925 - INFO - [Train] Epoch 9, Batch 21: Loss=0.029648
2024-12-30 18:39:01,173 - INFO - [Train] Epoch 9, Batch 22: Loss=0.037894
2024-12-30 18:39:01,724 - INFO - [Train] Epoch 9, Batch 23: Loss=0.034075
2024-12-30 18:39:02,259 - INFO - [Train] Epoch 9, Batch 24: Loss=0.034960
2024-12-30 18:39:22,992 - INFO - [Train] Epoch 9, Batch 25: Loss=0.028969
2024-12-30 18:39:23,469 - INFO - [Train] Epoch 9, Batch 26: Loss=0.043388
2024-12-30 18:39:23,943 - INFO - [Train] Epoch 9, Batch 27: Loss=0.036418
2024-12-30 18:39:24,450 - INFO - [Train] Epoch 9, Batch 28: Loss=0.033869
2024-12-30 18:39:24,955 - INFO - [Train] Epoch 9, Batch 29: Loss=0.032246
2024-12-30 18:39:25,452 - INFO - [Train] Epoch 9, Batch 30: Loss=0.030133
2024-12-30 18:39:25,989 - INFO - [Train] Epoch 9, Batch 31: Loss=0.033495
2024-12-30 18:39:26,504 - INFO - [Train] Epoch 9, Batch 32: Loss=0.031445
2024-12-30 18:39:43,041 - INFO - [Train] Epoch 9, Batch 33: Loss=0.027699
2024-12-30 18:39:43,647 - INFO - [Train] Epoch 9, Batch 34: Loss=0.030256
2024-12-30 18:39:44,243 - INFO - [Train] Epoch 9, Batch 35: Loss=0.027753
2024-12-30 18:39:44,816 - INFO - [Train] Epoch 9, Batch 36: Loss=0.035965
2024-12-30 18:39:45,479 - INFO - [Train] Epoch 9, Batch 37: Loss=0.029174
2024-12-30 18:39:46,013 - INFO - [Train] Epoch 9, Batch 38: Loss=0.029678
2024-12-30 18:39:46,615 - INFO - [Train] Epoch 9, Batch 39: Loss=0.065173
2024-12-30 18:39:47,231 - INFO - [Train] Epoch 9, Batch 40: Loss=0.031496
2024-12-30 18:39:48,862 - INFO - [Train] Epoch 9, Batch 41: Loss=0.154050
2024-12-30 18:39:49,378 - INFO - [Train] Epoch 9, Batch 42: Loss=0.029211
2024-12-30 18:39:49,886 - INFO - [Train] Epoch 9, Batch 43: Loss=0.035637
2024-12-30 18:39:57,279 - INFO - [Train] Epoch 9, Batch 44: Loss=0.030649
2024-12-30 18:39:57,766 - INFO - [Train] Epoch 9, Batch 45: Loss=0.032527
2024-12-30 18:40:01,407 - INFO - [Train] Epoch 9, Batch 46: Loss=0.039484
2024-12-30 18:40:01,839 - INFO - [Train] Epoch 9, Batch 47: Loss=0.029267
2024-12-30 18:40:02,285 - INFO - [Train] Epoch 9, Batch 48: Loss=0.027959
2024-12-30 18:40:02,928 - INFO - [Train] Epoch 9, Batch 49: Loss=0.028924
2024-12-30 18:40:03,351 - INFO - [Train] Epoch 9, Batch 50: Loss=0.147123
2024-12-30 18:40:04,880 - INFO - [Train] Epoch 9 completed -> Avg Loss: 0.037772
2024-12-30 18:40:04,880 - INFO - [Train] Epoch 9 completed -> Avg Loss: 0.037772
2024-12-30 18:40:04,894 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_9.pth
2024-12-30 18:40:04,894 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_9.pth
2024-12-30 18:40:04,894 - INFO - [Train] Epoch 10/10 started.
2024-12-30 18:40:34,758 - INFO - [Train] Epoch 10, Batch 1: Loss=0.028545
2024-12-30 18:40:35,216 - INFO - [Train] Epoch 10, Batch 2: Loss=0.037951
2024-12-30 18:40:38,775 - INFO - [Train] Epoch 10, Batch 3: Loss=0.034654
2024-12-30 18:40:39,230 - INFO - [Train] Epoch 10, Batch 4: Loss=0.029384
2024-12-30 18:40:39,691 - INFO - [Train] Epoch 10, Batch 5: Loss=0.031748
2024-12-30 18:40:40,138 - INFO - [Train] Epoch 10, Batch 6: Loss=0.034975
2024-12-30 18:40:40,585 - INFO - [Train] Epoch 10, Batch 7: Loss=0.035021
2024-12-30 18:40:41,069 - INFO - [Train] Epoch 10, Batch 8: Loss=0.028327
2024-12-30 18:40:47,311 - INFO - [Train] Epoch 10, Batch 9: Loss=0.030892
2024-12-30 18:40:51,724 - INFO - [Train] Epoch 10, Batch 10: Loss=0.032609
2024-12-30 18:41:06,609 - INFO - [Train] Epoch 10, Batch 11: Loss=0.036270
2024-12-30 18:41:07,152 - INFO - [Train] Epoch 10, Batch 12: Loss=0.033581
2024-12-30 18:41:07,641 - INFO - [Train] Epoch 10, Batch 13: Loss=0.031455
2024-12-30 18:41:08,171 - INFO - [Train] Epoch 10, Batch 14: Loss=0.027513
2024-12-30 18:41:08,634 - INFO - [Train] Epoch 10, Batch 15: Loss=0.036341
2024-12-30 18:41:09,101 - INFO - [Train] Epoch 10, Batch 16: Loss=0.036920
2024-12-30 18:41:09,565 - INFO - [Train] Epoch 10, Batch 17: Loss=0.028101
2024-12-30 18:41:10,974 - INFO - [Train] Epoch 10, Batch 18: Loss=0.027487
2024-12-30 18:41:24,136 - INFO - [Train] Epoch 10, Batch 19: Loss=0.032114
2024-12-30 18:41:24,588 - INFO - [Train] Epoch 10, Batch 20: Loss=0.031668
2024-12-30 18:41:25,108 - INFO - [Train] Epoch 10, Batch 21: Loss=0.032007
2024-12-30 18:41:25,650 - INFO - [Train] Epoch 10, Batch 22: Loss=0.030000
2024-12-30 18:41:36,058 - INFO - [Train] Epoch 10, Batch 23: Loss=0.029591
2024-12-30 18:41:36,508 - INFO - [Train] Epoch 10, Batch 24: Loss=0.032508
2024-12-30 18:41:36,958 - INFO - [Train] Epoch 10, Batch 25: Loss=0.035794
2024-12-30 18:41:37,430 - INFO - [Train] Epoch 10, Batch 26: Loss=0.032128
2024-12-30 18:41:43,344 - INFO - [Train] Epoch 10, Batch 27: Loss=0.037047
2024-12-30 18:41:43,806 - INFO - [Train] Epoch 10, Batch 28: Loss=0.029457
2024-12-30 18:41:44,264 - INFO - [Train] Epoch 10, Batch 29: Loss=0.138537
2024-12-30 18:41:44,716 - INFO - [Train] Epoch 10, Batch 30: Loss=0.034836
2024-12-30 18:41:57,626 - INFO - [Train] Epoch 10, Batch 31: Loss=0.027434
2024-12-30 18:41:58,092 - INFO - [Train] Epoch 10, Batch 32: Loss=0.028374
2024-12-30 18:41:58,566 - INFO - [Train] Epoch 10, Batch 33: Loss=0.037604
2024-12-30 18:41:59,039 - INFO - [Train] Epoch 10, Batch 34: Loss=0.034002
2024-12-30 18:41:59,521 - INFO - [Train] Epoch 10, Batch 35: Loss=0.031432
2024-12-30 18:42:00,016 - INFO - [Train] Epoch 10, Batch 36: Loss=0.067463
2024-12-30 18:42:00,484 - INFO - [Train] Epoch 10, Batch 37: Loss=0.029737
2024-12-30 18:42:00,982 - INFO - [Train] Epoch 10, Batch 38: Loss=0.131296
2024-12-30 18:42:14,556 - INFO - [Train] Epoch 10, Batch 39: Loss=0.030087
2024-12-30 18:42:15,054 - INFO - [Train] Epoch 10, Batch 40: Loss=0.031276
2024-12-30 18:42:15,570 - INFO - [Train] Epoch 10, Batch 41: Loss=0.031822
2024-12-30 18:42:16,066 - INFO - [Train] Epoch 10, Batch 42: Loss=0.027432
2024-12-30 18:42:16,571 - INFO - [Train] Epoch 10, Batch 43: Loss=0.034497
2024-12-30 18:42:17,079 - INFO - [Train] Epoch 10, Batch 44: Loss=0.036838
2024-12-30 18:42:17,588 - INFO - [Train] Epoch 10, Batch 45: Loss=0.027670
2024-12-30 18:42:18,083 - INFO - [Train] Epoch 10, Batch 46: Loss=0.030439
2024-12-30 18:42:33,454 - INFO - [Train] Epoch 10, Batch 47: Loss=0.030917
2024-12-30 18:42:33,977 - INFO - [Train] Epoch 10, Batch 48: Loss=0.032914
2024-12-30 18:42:34,472 - INFO - [Train] Epoch 10, Batch 49: Loss=0.029035
2024-12-30 18:42:34,936 - INFO - [Train] Epoch 10, Batch 50: Loss=0.031954
2024-12-30 18:42:37,571 - INFO - [Train] Epoch 10 completed -> Avg Loss: 0.036794
2024-12-30 18:42:37,571 - INFO - [Train] Epoch 10 completed -> Avg Loss: 0.036794
2024-12-30 18:42:37,586 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_10.pth
2024-12-30 18:42:37,587 - INFO - [Train] Saved checkpoint: C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\CheckPoints\model_epoch_10.pth
2024-12-30 18:43:42,847 - INFO - [Train] Training completed. Cleared memory cache.
2024-12-30 18:43:42,847 - INFO - [Train] Training completed. Cleared memory cache.
2024-12-30 18:43:42,861 - INFO - [Train] Final model saved at C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\Final_model\Final_model2.pth
2024-12-30 18:43:42,861 - INFO - [Train] Final model saved at C:\Users\didri\Desktop\LearnReflect VideoEnchancer\AI UNet-Architecture\UNet_Model\Final_model\Final_model2.pth
